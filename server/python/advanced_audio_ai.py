import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import os  # æ·»åŠ osæ¨¡å—å¯¼å…¥
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D
from tensorflow.keras.utils import to_categorical
import warnings
warnings.filterwarnings('ignore')

class AdvancedAudioAI:
    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=20)
        self.label_encoder = LabelEncoder()
        self.models = {
            'random_forest': RandomForestClassifier(n_estimators=200, random_state=42),
            'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'neural_network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
        }
        self.deep_model = None
        self.feature_names = []
        
    def extract_advanced_features(self, audio_path):
        """æå–é«˜çº§éŸ³é¢‘ç‰¹å¾"""
        print(f"ğŸ” æ­£åœ¨æå–é«˜çº§ç‰¹å¾: {audio_path}")
        
        audio, sr = librosa.load(audio_path)
        
        features = {}
        
        # 1. åŸºç¡€ç‰¹å¾
        features['duration'] = librosa.get_duration(y=audio, sr=sr)
        features['sample_rate'] = sr
        features['n_samples'] = len(audio)
        
        # 2. é¢‘è°±ç‰¹å¾
        # æ¢…å°”é¢‘è°±å›¾
        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
        features['mel_spectrogram_mean'] = np.mean(mel_spec)
        features['mel_spectrogram_std'] = np.std(mel_spec)
        features['mel_spectrogram_max'] = np.max(mel_spec)
        features['mel_spectrogram_min'] = np.min(mel_spec)
        
        # 3. MFCCç‰¹å¾ (æ›´å¤šç³»æ•°)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
        for i in range(20):
            features[f'mfcc_{i}_mean'] = np.mean(mfccs[i])
            features[f'mfcc_{i}_std'] = np.std(mfccs[i])
            features[f'mfcc_{i}_max'] = np.max(mfccs[i])
            features[f'mfcc_{i}_min'] = np.min(mfccs[i])
        
        # 4. èŠ‚å¥å’ŒèŠ‚æ‹ç‰¹å¾
        tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)
        features['tempo'] = tempo
        features['beat_count'] = len(beats)
        features['beat_interval_mean'] = np.mean(np.diff(beats)) if len(beats) > 1 else 0
        
        # 5. éŸ³é«˜ç‰¹å¾
        pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
        features['pitch_mean'] = np.mean(pitches)
        features['pitch_std'] = np.std(pitches)
        features['pitch_max'] = np.max(pitches)
        features['pitch_min'] = np.min(pitches)
        
        # 6. èƒ½é‡ç‰¹å¾
        rms = librosa.feature.rms(y=audio)
        features['rms_mean'] = np.mean(rms)
        features['rms_std'] = np.std(rms)
        features['rms_max'] = np.max(rms)
        features['rms_min'] = np.min(rms)
        
        # 7. é›¶äº¤å‰ç‡
        zcr = librosa.feature.zero_crossing_rate(audio)
        features['zcr_mean'] = np.mean(zcr)
        features['zcr_std'] = np.std(zcr)
        features['zcr_max'] = np.max(zcr)
        
        # 8. é¢‘è°±ç‰¹å¾
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)
        features['spectral_centroid_mean'] = np.mean(spectral_centroids)
        features['spectral_centroid_std'] = np.std(spectral_centroids)
        features['spectral_centroid_max'] = np.max(spectral_centroids)
        
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)
        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)
        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)
        
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)
        features['spectral_rolloff_std'] = np.std(spectral_rolloff)
        
        # 9. è°æ³¢å’Œæ‰“å‡»ä¹åˆ†ç¦»
        harmonic, percussive = librosa.effects.hpss(audio)
        features['harmonic_ratio'] = np.sum(harmonic**2) / (np.sum(harmonic**2) + np.sum(percussive**2))
        features['percussive_ratio'] = 1 - features['harmonic_ratio']
        
        # 10. è‰²åº¦ç‰¹å¾
        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
        features['chroma_mean'] = np.mean(chroma)
        features['chroma_std'] = np.std(chroma)
        
        # 11. éŸ³è°ƒç‰¹å¾
        tonnetz = librosa.feature.tonnetz(y=harmonic, sr=sr)
        features['tonnetz_mean'] = np.mean(tonnetz)
        features['tonnetz_std'] = np.std(tonnetz)
        
        # 12. é¢‘è°±å¯¹æ¯”åº¦
        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
        features['spectral_contrast_mean'] = np.mean(contrast)
        features['spectral_contrast_std'] = np.std(contrast)
        
        # 13. å¤šæ™®å‹’ç‰¹å¾
        poly_features = librosa.feature.poly_features(y=audio, sr=sr)
        features['poly_features_mean'] = np.mean(poly_features)
        features['poly_features_std'] = np.std(poly_features)
        
        return features
    
    def create_deep_learning_model(self, input_shape, num_classes):
        """åˆ›å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        model = Sequential([
            # å·ç§¯å±‚
            Conv1D(64, 3, activation='relu', input_shape=input_shape),
            MaxPooling1D(2),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='relu'),
            MaxPooling1D(2),
            
            # LSTMå±‚
            LSTM(128, return_sequences=True),
            Dropout(0.3),
            LSTM(64),
            Dropout(0.3),
            
            # å…¨è¿æ¥å±‚
            Dense(128, activation='relu'),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dense(num_classes, activation='softmax')
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_deep_learning_data(self, audio_paths, labels):
        """å‡†å¤‡æ·±åº¦å­¦ä¹ æ•°æ®"""
        print("ğŸ”„ å‡†å¤‡æ·±åº¦å­¦ä¹ æ•°æ®...")
        
        X_deep = []
        y_deep = []
        
        for audio_path in audio_paths:
            try:
                audio, sr = librosa.load(audio_path, sr=22050)
                
                # æå–MFCCç‰¹å¾ä½œä¸ºæ·±åº¦å­¦ä¹ è¾“å…¥
                mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
                
                # ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
                if mfccs.shape[1] > 1000:
                    mfccs = mfccs[:, :1000]
                elif mfccs.shape[1] < 1000:
                    # å¡«å……åˆ°1000
                    padding = np.zeros((13, 1000 - mfccs.shape[1]))
                    mfccs = np.hstack([mfccs, padding])
                
                X_deep.append(mfccs.T)  # è½¬ç½®ä»¥åŒ¹é…Conv1Dè¾“å…¥æ ¼å¼
                
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {audio_path}: {e}")
                continue
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(labels[:len(X_deep)])
        y_categorical = to_categorical(y_encoded)
        
        return np.array(X_deep), y_categorical
    
    def train_models(self, audio_paths, labels):
        """è®­ç»ƒå¤šä¸ªAIæ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒAIæ¨¡å‹...")
        
        # æå–ç‰¹å¾
        features_list = []
        valid_labels = []
        
        for i, audio_path in enumerate(audio_paths):
            try:
                features = self.extract_advanced_features(audio_path)
                features_list.append(features)
                valid_labels.append(labels[i])
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {audio_path}: {e}")
                continue
        
        if len(features_list) < 2:
            print("âŒ æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(features_list)
        self.feature_names = df.columns.tolist()
        
        # å‡†å¤‡æ•°æ®
        X = df.values
        y = np.array(valid_labels)
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        # æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        results = {}
        for name, model in self.models.items():
            print(f"ğŸ”„ è®­ç»ƒ {name}...")
            model.fit(X_train_scaled, y_train)
            
            # è¯„ä¼°
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            results[name] = {
                'accuracy': accuracy,
                'predictions': y_pred,
                'model': model
            }
            print(f"   {name} å‡†ç¡®ç‡: {accuracy:.3f}")
        
        # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        print("ğŸ”„ è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        X_deep, y_deep = self.prepare_deep_learning_data(audio_paths, labels)
        
        if len(X_deep) > 0:
            X_deep_train, X_deep_test, y_deep_train, y_deep_test = train_test_split(
                X_deep, y_deep, test_size=0.2, random_state=42
            )
            
            self.deep_model = self.create_deep_learning_model(
                input_shape=(X_deep.shape[1], X_deep.shape[2]),
                num_classes=y_deep.shape[1]
            )
            
            history = self.deep_model.fit(
                X_deep_train, y_deep_train,
                epochs=50,
                batch_size=32,
                validation_data=(X_deep_test, y_deep_test),
                verbose=1
            )
            
            # è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹
            deep_accuracy = self.deep_model.evaluate(X_deep_test, y_deep_test)[1]
            results['deep_learning'] = {
                'accuracy': deep_accuracy,
                'model': self.deep_model,
                'history': history
            }
            print(f"   æ·±åº¦å­¦ä¹ æ¨¡å‹å‡†ç¡®ç‡: {deep_accuracy:.3f}")
        
        return results, X_test, y_test
    
    def analyze_audio_cluster(self, audio_paths):
        """ä½¿ç”¨èšç±»åˆ†æéŸ³é¢‘ç‰¹å¾"""
        print("ğŸ” æ‰§è¡Œèšç±»åˆ†æ...")
        
        # æå–ç‰¹å¾
        features_list = []
        valid_paths = []
        
        for audio_path in audio_paths:
            try:
                features = self.extract_advanced_features(audio_path)
                features_list.append(features)
                valid_paths.append(audio_path)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {audio_path}: {e}")
                continue
        
        if len(features_list) < 2:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(features_list)
        X = df.values
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # é™ç»´
        # Adjust n_components to be at most the minimum of samples and features
        n_components = min(2, min(X_scaled.shape[0], X_scaled.shape[1]))
        self.pca = PCA(n_components=n_components)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=min(5, len(X_pca)), random_state=42)
        clusters = kmeans.fit_predict(X_pca)
        
        # å¯è§†åŒ–èšç±»ç»“æœ
        plt.figure(figsize=(12, 8))
        
        # ä¸»æˆåˆ†åˆ†æå¯è§†åŒ–
        plt.subplot(2, 2, 1)
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
        plt.title('PCA + K-means Clustering')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')
        plt.colorbar(scatter)
        
        # ç‰¹å¾é‡è¦æ€§
        plt.subplot(2, 2, 2)
        feature_importance = np.abs(self.pca.components_[0])
        top_features_idx = np.argsort(feature_importance)[-10:]
        plt.barh(range(10), feature_importance[top_features_idx])
        plt.yticks(range(10), [df.columns[i] for i in top_features_idx])
        plt.title('Top 10 Feature Importance')
        plt.xlabel('Importance')
        
        # èšç±»ç»Ÿè®¡
        plt.subplot(2, 2, 3)
        cluster_counts = np.bincount(clusters)
        plt.bar(range(len(cluster_counts)), cluster_counts)
        plt.title('Cluster Distribution')
        plt.xlabel('Cluster')
        plt.ylabel('Count')
        
        # ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾
        plt.subplot(2, 2, 4)
        correlation_matrix = df.corr()
        sns.heatmap(correlation_matrix.iloc[:10, :10], annot=True, cmap='coolwarm', center=0)
        plt.title('Feature Correlation Matrix')
        
        plt.tight_layout()
        plt.show()
        
        return clusters, df
    
    def generate_ai_report(self, audio_paths, labels=None):
        """ç”ŸæˆAIåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ¤– é«˜çº§éŸ³é¢‘AIåˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # èšç±»åˆ†æ
        clusters, features_df = self.analyze_audio_cluster(audio_paths)
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œè®­ç»ƒæ¨¡å‹
        if labels:
            results, X_test, y_test = self.train_models(audio_paths, labels)
            
            # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½
            print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
            for name, result in results.items():
                print(f"   {name:15s}: {result['accuracy']:.3f}")
        
        # ç‰¹å¾ç»Ÿè®¡
        print(f"\nğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
        print(f"   åˆ†ææ–‡ä»¶æ•°: {len(audio_paths)}")
        print(f"   æå–ç‰¹å¾æ•°: {len(features_df.columns)}")
        print(f"   èšç±»æ•°é‡: {len(np.unique(clusters))}")
        
        # ä¿å­˜ç»“æœ
        features_df['cluster'] = clusters
        features_df.to_csv('advanced_audio_analysis.csv', index=False)
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ° advanced_audio_analysis.csv")
        
        return features_df

def main():
    # åˆ›å»ºé«˜çº§AIåˆ†æå™¨
    ai_analyzer = AdvancedAudioAI()
    
    # éŸ³ä¹ç›®å½•è·¯å¾„
    music_dir = 'C:\\Users\\eggy2\\wyc\\æ•°å­¦è®ºæ–‡\\éŸ³ä¹'
    
    # æ”¶é›†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    audio_files = []
    audio_extensions = ['.mp3', '.ogg', '.wav', '.flac']  # æ”¯æŒçš„éŸ³é¢‘æ‰©å±•å
    
    for root, dirs, files in os.walk(music_dir):
        for file in files:
            if any(file.lower().endswith(ext) for ext in audio_extensions):
                file_path = os.path.join(root, file)
                audio_files.append(file_path)
    
    print(f"ğŸ” å‘ç° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    try:
        # ç”ŸæˆAIåˆ†ææŠ¥å‘Šï¼ˆæ— æ ‡ç­¾ï¼‰
        results = ai_analyzer.generate_ai_report(audio_files)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print("è¯·ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")

if __name__ == "__main__":
    main()